## 								 KP之K-means

### 1.Kmeans

![](E:\ke\Pictures\K-means.png)

- #### 1.1 简介

  给定数据点集和与聚类数目k，K-means算法会根据欧式距离将数据点集合分成k类

- #### 1.2 原理

  1. 随机选取k个点作为聚类中心

  2. 计算每个点到各个聚类中心的距离，并将该点分配至其距离最近的聚类中心所在类

  3. 更新聚类中心。计算每类包含数据点的平均值，并作为该类新的聚类中心

  4. 重复(2)(3)步骤，直至收敛（最大迭代次数或聚类中心不再变化）

     

  - **伪代码**

    ```c++
    输入n个m维数据
    输入聚类个数k
    while(t)
        for(int i=0;i < n;i++)
            for(int j=0;j < k;j++)
                计算点 i 到类 j 的距离
            将点 i 分类到其最近的聚类中心
        for(int i=0;i < k;i++)
            a. 找出所有属于自己这一类的所有数据点
            b. 计算平均值，更新聚类中心的坐标
    end
            
    算法时间复杂度O(tmnk)
    ```

- #### 1.3 算法优化

  ​	K-means算法原理比较简单，聚类效果也不错。但当数据量大时，计算复杂度高，迭代次数多。

  为了减少计算时间，要么减少数据量，要么减少迭代次数

  ①mini batch

  ​	既然数据量太大，使得计算时间过长，那么我们可以减少计算数据规模。我们随机从整体中抽取一部分数据用来代替整体，这样计算速度就提升了。当然这样存在随机性，抽取到的样本可能都在原本数据集的一个簇中，使得结果误差非常大。所以我们需要采用多次随机，并将每次随机得到的数据都进行一次K-means训练，将每次训练得到的k个聚类中心取平均，直至簇中心趋于稳定。

  ②K-means++

  ​	除了减少计算量之外，我们可以通过减少迭代次数的方法来加快训练。在基础K-means中，我们的聚类中心是随机选取的，在++中，我们在选择在样本中随机选取k个点作为聚类中心原因也很简单，因为我们随机坐标对应的是在最大和最小值框成的矩形面积当中选择K个点，而我们从样本当中选K个点的范围则要小得多

  ​	此外,k个点并非完全随机,一般来说,两个聚簇中心的距离相对来说是比较远的.所以我们在选取了第一个聚簇中心之后,会进行一次计算,计算省下所有节点与其最近的聚簇中心距离,然后通过轮盘算法进行抽取下一个中心.

  ​	实验证明这个会比较好,最坏情况也不会很坏



- #### Tips

​			K值选取

​			k值的选取对K-means训练结果影响很大,可以通过肘部法来选取k值,即将k在不同取值下的代价函数结果输出,选取拐点

​			![image-20210716204407657](C:\Users\bingchengke\AppData\Roaming\Typora\typora-user-images\image-20210716204407657.png)

​				可视化

​				很多时候肘部法也不起作用,k值选取还是需要通过经验决定,通过可视化结果评定选值好坏

2.KP平台使用Kmeans

​	输入

​	参数

​	结果分析